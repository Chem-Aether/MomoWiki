# 朴素贝叶斯法

## 一些概率论基础

### 条件概率
联合概率分布描述的是多个随机变量同时取特定值的概率。对于一组随机变量$x_1,x_2,…,x_n$，其联合概率分布表示为：

$$ P(X_1=x_1,X_2=x_2, \cdots,X_n=x_n) ，或简写为：P(x_1,x_2, \cdots,x_n) $$

### 联合概率分布

$$ P(X,Y) = P(X | Y) \times P(Y) $$


### 贝叶斯定理

贝叶斯定理是**概率论**中的核心定理，描述了在观察到新证据（数据）后，如何更新对事件的**先验概率**（初始信念）以得到**后验概率**（修正后的信念）。它是**贝叶斯统计**和**机器学习**（如朴素贝叶斯分类、贝叶斯网络）的基础。

$$  P(A|B) ={ { P(B|A) \cdot P(A) } \over P(B) } $$

## 基本方法

训练数据集：$T = \{ (x_1,y_1),(x_2,y_2), \cdots,(x_n,y_n) \}$

先验概率分布：$P(Y=c_k),\quad k=1,2, \cdots,K$

条件概率分布：
$$
\begin{align*}
P(X=x|Y=c_k) &=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)}, \cdots,X^{(n)}=x^{(n)}|Y=c_k )\\
k&=1,2, \cdots,K
\end{align*}
$$

条件独立性假设：
$$
\begin{align*}
P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)}, \cdots,X^{(n)}=x^{(n)}|Y=c_k )
\\
&=\prod\limits_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)
\end{align*}
$$

朴素贝叶斯分类器的表示：
$$
\begin{align*}
y=f(x)&={ {P(X=x|Y=c_k) \cdot P(Y=c_k) } \over {\sum\limits_k P(X=x|Y=c_k) \cdot P(Y=c_k)} }\\
&=\arg \max_{c_k} { {P(Y=c_k) \cdot \prod\limits_j P(X^{(j)}=x^{(j)}|Y=c_k)} \over \sum\limits_k P(Y=c_k) \cdot \prod\limits_j P(X^{(j)}=x^{(j)}|Y=c_k) }
\end{align*}
$$

去掉相同的分母，则：
$$
y=f(x)=\arg \max_{c_k} {P(Y=c_k) \cdot \prod\limits_j P(X^{(j)}=x^{(j)}|Y=c_k)}
$$

后验概率最大化

选择0-1损失函数：
$$
L(Y,f(X)) = \left\{
\begin{array}{rl}1, & Y \neq f(X)\\
0, & Y=f(X)\\
\end{array}\right.
$$

期望风险函数：
$$
R_{\exp} (f) = E[L(Y,f(X))]
$$

条件期望：
$$
R_{\exp} (f) = E_X \sum\limits_{k=1}^K [L(c_k,f(X))] \cdot P(c_k|X=x)
$$

对$X=x$逐个极小化，得到期望风险最小化：
$$
\begin{align*}
f(x) &= \arg\min_{y \in \mathcal{Y}} \sum_{k=1}^{K} L(c_k, y) P(c_k \mid X = x) \\
&= \arg\min_{y \in \mathcal{Y}} \sum_{k=1}^{K} P(y \neq c_k \mid X = x) \\
&= \arg\min_{y \in \mathcal{Y}} \left(1 - P(y = c_k \mid X = x)\right) \\
&= \arg\max_{y \in \mathcal{Y}} P(y = c_k \mid X = x)
\end{align*}
$$

得到后验概率最大化准则：
$$
f(x)=\arg \max_{c_k} P(c_k|X=x)
$$

极大似然估计

## 算法实现

贝叶斯估计