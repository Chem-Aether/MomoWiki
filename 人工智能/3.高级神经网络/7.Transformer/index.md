# Transformers

大名鼎鼎的Transformer模型最早是谷歌DeepMind在2017年的论文《Attention Is All You Need》首次提出 Transformer 架构。这篇论文摒弃了传统的循环结构，完全基于注意力机制构建了一个全新的模型架构。本节讲详细讲述Transformer模型的架构流程。

## 概述

原文中通过一张图清晰地展示了该模型的架构，如图，它主要由编码器和解码器两个部分组成，编码器和解码器又分别是N个相同模块的堆叠。

整个体系实际上是由`多头注意力层`、`残差连接层`、`归一化层`、`前馈网络层`的排列组合构成。

## 文字嵌入

对于长度为$N$的输入序列，可以将其转变为维度为$C$的嵌入向量$w=[c_1,c_2,\cdots,c_C]$。

整个输入序列可以用输入矩阵表示：$W=[w_1,w_2,\cdots,w_N]$

## QKV计算
QKV矩阵的计算是注意力的核心，

对于单个字词的注意力计算示意图如下：



随后，可以利用矩阵运算的性质，将输入的序列“打包”成一个矩阵，进行计算：

## 缩放点积注意力模块

缩放点积注意力算法是自注意力的一种实现形式，它通过如下公式计算：

$$Attention = softmax(\frac{QK^T}{\sqrt{d}})V$$



## 多头注意力模块

## 掩膜注意力模块

## 编码器

## 解码器