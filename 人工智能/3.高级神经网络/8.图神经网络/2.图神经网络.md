# GNN

## GCN

$h_u^{(t)}$：节点u在第t层的特征表示。

$\mathcal{N}(v)$：节点v的一阶邻居集合（不包含自身）。

$\hat{d}_u, \hat{d}_v$：节点$u, v$的度（包含自环，即节点自身与自身的连接）。

$W^{(t)}$：第t层的可学习权重矩阵。$\sigma$：非线性激活函数（如 ReLU）。


在第$t+1$层，邻居节点$u$的特征$h_u^{(t)}$作为消息传入中心节点$v$：

$$
m_{u,v}^{(t+1)} = h_u^{(t)}
$$

所有邻居的归一化消息求和，再加上中心节点v自身在第t层的特征$h_v^{(t)}$

$$
m_v^{(t+1)} = \sum_{u \in \mathcal{N}(v)} \frac{1}{\sqrt{ \hat{d}_u \hat{d}_v }} m_{u,v}^{(t+1)} + h_v^{(t)} 
$$

将节点总消息传入MLP并进行非线性激活，更新节点得到$h_v^{(t+1)}$

$$
h_v^{(t+1)} = \sigma\left( W^{(t)} m_v^{(t+1)} \right)
$$


为了简化表达，我们将 “自环”（节点自身与自身的连接）纳入邻居集合$\mathcal{N}(v) \cup \{v\}$，此时聚合过程可直接包含自身特征，无需额外加$h_v^{(t)}$。
代入消息生成公式$m_{uv}^{(t+1)} = h_u^{(t)}$到聚合和更新步骤中：
$$
\begin{align*}
h_v^{(t+1)} &= \sigma\left( W^{(t)} \left( \sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{\hat{d}_u \hat{d}_v}} h_u^{(t)} \right) \right) \\
&= \sigma\left( \sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{\hat{d}_u \hat{d}_v}} W^{(t)} h_u^{(t)} \right)
\end{align*}
$$
  
这就是最终的紧凑形式：
$$
h_v^{(t+1)} = \sigma\left( \sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{\hat{d}_u \hat{d}_v}} W^{(t)} h_u^{(t)} \right)
$$

## GraphSAGE

聚合函数公式：

$$
m_v^{(t+1)} = \text{Agg}(m_{uv}^{(t+1)} : u \in \mathcal{N}(v))
$$

节点更新
$$
h_v^{(t+1)} = \sigma\left( W^{(t)} \left[ h_v^t || m_v^{(t+1)} \right] \right)
$$

## GIN

公式：
$$
m_v^{(t+1)} = \sum_{(u, v) \in E} h_u^{(t)}
$$

含义：中心节点v收集所有以自身为终点的边$(u, v)$对应的消息（即所有邻居u的旧特征），并对这些消息进行求和聚合。逻辑：将所有邻居的特征 “加总”，得到邻居信息的整体表示$m_v^{(t+1)}$。

节点更新（Update）公式：

$$
h_v^{(t+1)} = \text{MLP}\left( (1 + \epsilon^{(t+1)}) h_v^{(t)} + m_v^{(t+1)} \right)
$$

## GAT

公式：
$$
e_{uv} = \mathbf{a}^T \text{LeakyReLU}(W h_u + W h_v)
$$

对节点$u$和$v$的特征分别应用线性变换$W$，得到$Wh_u$和$Wh_v$（目的是将特征映射到共享空间，便于计算相似度）。

将两者相加（$W h_u + W h_v$），再通过$\text{LeakyReLU}$激活，引入非线性。

与注意力向量$\mathbf{a}$做内积（$\mathbf{a}^T \cdot (\dots)$），得到节点对$(u, v)$的原始注意力分数$e_{uv}$。



公式：
$$
\alpha_{uv} = \frac{\exp(e_{uv})}{\sum_{k \in \mathcal{N}(v)} \exp(e_{vk})}
$$


公式：
$$
m_{uv}^{(t+1)} = \alpha_{uv} h_u^{(t)}
$$



公式：
$$
m_v^{(t+1)} = \sum_{u \in \mathcal{N}(v)} m_{uv}^{(t+1)}
$$



公式：
$$
h_v^{(t+1)} = \sigma\left( W^{(t)} m_v^{(t+1)} \right)
$$



