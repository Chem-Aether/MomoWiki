# 基于DDPM实现图像生成项目

项目结构如下：

```
data                        # 数据集下载目录
├──MNIST
├──FASHION_MNIST
└──CIFAR10
model                       # 模型实现
└── PTH                     # 训练完成的权重
|   ├── number_model.pt     # MNIST数据集结果
|   ├── fashion_model.pt    # FASHION_MNIST结果
|   └── CIFAR_model.pt      # CIFAR10数据集结果
├── config.py               # 配置文件
├── conv_block.py           # 卷积块
├── dataset.py              # 数据集定义
├── denoise.py              # 去噪
├── noise.py                # 加噪
├── TimeEmbedding.py        # 时间步嵌入生成
└── unet.py                 # Unet网络定义
train.py                    # 训练入口
inference.py                # 推理入口
```


## 超参数配置

这部分代码由**config.py**文件实现，主要定义了DDPM的各种超参数：

```python
import torch 

# === 数据集相关参数 ===
DATASET_NAME = "huggan/flowers-102-categories"
LOCAL_CACHE_DIR = "./data"

# === 图像相关参数 ===
IMG_SIZE=32             # 图像尺寸
IMG_CHANNEL= 1          # 图像通道数，彩色图像为3，灰度图像为1

# === 训练相关参数 ===
T=1000                  # 加噪最大步数
TIME_EMB_SIZE=256       # Time时刻emb的维度
EPOCH=1000              # 训练轮数
BATCH_SIZE=64           # 批次大小
LEARNING_RATE = 1e-4    # 学习率

# 训练设备
DEVICE = "cuda" if torch.cuda.is_available() else "cpu" 

# 模型保存路径
MODEL_PTH = 'CIFAR_model.pt'  
```

## 数据集载入

这部分代码由**dataset.py**文件实现，主要完成图像数据与Tensoer互相转换、数据集载入：

PIL转Tensoer
```python
pil_to_tensor=transforms.Compose([
    # PIL图像尺寸统一  
    transforms.Resize((IMG_SIZE,IMG_SIZE)),
    # PIL图像转tensor, (H,W,C) ->（C,H,W）,像素值[0,1]s
    transforms.ToTensor()
])
```
Tensoer转PIL
```python
tensor_to_pil=transforms.Compose([
    # 像素还原
    transforms.Lambda(lambda t: t*255),
    # 像素值取整
    transforms.Lambda(lambda t: t.type(torch.uint8)),
    # tensor转回PIL图像, (C,H,W) -> (H,W,C) 
    transforms.ToPILImage(),
])
```
加载数据集
```python
# 加载数据集，这会自动下载并缓存
user_dataset = load_dataset(
    DATASET_NAME,               # 数据集名称
    split="train",              # 选择数据集的一个子集，这里选择训练集
    cache_dir=LOCAL_CACHE_DIR   # 指定缓存目录
    )

# 将PIL图像转换为tensor格式，并调整数据集格式以便PyTorch使用
user_dataset = user_dataset.map(
    lambda example: {'image': pil_to_tensor(example['image'])}, 
    batched=False
    )

# 转为torch tensor，只保留'image'列
user_dataset.set_format(type='torch',columns=['image'])
```
数据集的加载（三选一）：
```python
user_dataset=torchvision.datasets.MNIST(
    root=".",
    train=True,
    download=True,
    transform=pil_to_tensor
    ) 

user_dataset = torchvision.datasets.CIFAR10(
    './data', 
    train=True, 
    download=True, 
    transform=pil_to_tensor
    )

user_dataset = torchvision.datasets.FashionMNIST(
    './data', 
    train=True, 
    download=True, 
    transform=pil_to_tensor
    )
```

模块测试，随机选择100条数据进行可视化：
```python
if __name__=='__main__':
    print("数据集信息:")
    print(user_dataset)

    # 随机选择100个样本
    indices = random.sample(range(len(user_dataset)), 100)

    # 绘制10x10网格
    plt.figure(figsize=(10, 10))

    for i, idx in enumerate(indices):
        image, label = user_dataset[idx]
        pil_image = tensor_to_pil(image)
        
        plt.subplot(10, 10, i + 1)
        plt.imshow(pil_image)
        plt.axis('off')

    plt.tight_layout()
    plt.show()
```

以**FASHION_MNIST**数据集为例，输出如下：
![](fashion_mnist.png)

## 加噪

根据加噪公式，直接计算第$t$时刻的加噪后图像：

$$
x_t = \sqrt{\bar{\alpha}_t}\cdot x_0 + \sqrt{1-\bar{\alpha}_t} \cdot \varepsilon
$$

首先定义以下超参数：
- $\alpha_t = 1- \beta_t, \quad \beta_t,t \in [0,T]$

- $\bar{\alpha}_t = \prod_T \alpha_t$

```python
# 超参数定义
# 线性beta调度器，beta_t从0.0001线性增加到0.02
betas=torch.linspace(0.0001,0.02,T)
# 计算alpha
alphas=1-betas
# 计算各时间步的alpha累乘 [a1,a1*a2,a1*a2*a3,.....]
alphas_cumprod=torch.cumprod(alphas,dim=-1) # alpha_t累乘 (T,)    
# 计算各时间步的alpha累乘的前一个时间步 [1,a1,a1*a2,a1*a2*a3,.....]
alphas_cumprod_prev=torch.cat(
    (
        torch.tensor([1.0]),alphas_cumprod[:-1]),
        dim=-1
    )
```
定义前向加噪函数
```python
# 前向加噪函数
# batch_x: (batch,channel,width,height), batch_t: (batch_size,)
def forward_diffusion(batch_x,batch_t): 
    # 生成第t步的高斯噪音   (batch,channel,width,height)
    batch_noise_t=torch.randn_like(batch_x)
    # 将系数维数调整到和batch一致
    batch_alphas_cumprod=alphas_cumprod.to(DEVICE)[batch_t].view(batch_x.size(0),1,1,1) 
    # 按公式加噪
    batch_x_t=torch.sqrt(batch_alphas_cumprod)*batch_x+torch.sqrt(1-batch_alphas_cumprod)*batch_noise_t
    return batch_x_t,batch_noise_t
```

## 时间步嵌入
回顾时间步向量公式：
$$
\begin{align*}
& PE(t,2i) = sin(\frac{t}{10000^k}) \\
& PE(t,2i+1) = cos(\frac{t}{10000^k}) \\
& k=\frac{2i}{d_{model}}
\end{align*}
$$

构建该向量需要向量维数 $d_{model}$，时间步 $t$，计算向量每个元素时需要位置索引 $i$。

以一个十维向量$d_{model}=10$为例，它第$t=100$时的向量可计算为：

|index|0|1|2|3|4|5|6|7|8|9|
|---|---|---|---|---|---|---|---|---|---|---|
|i|0|0|1|1|2|2|3|3|4|4|
|k|0|0|0.2|0.2|0.4|0.4|0.6|0.6|0.8|0.8|
|$PE(t,x)$|sin0|cos0|sin0.2|cos0.2|sin0.4|cos0.4|sin0.6|cos0.6|sin0.8|co0.8|

其中：$i=\frac{index}{2}/\frac{index-1}{2}$，$k=\frac{2i}{d_{model}}$

观察发现时间向量两两成对，在奇数时利用$sin x$，偶数时利用$cosx$，因此我们可以创建一个大小为$d_{model}/2$的索引序列，生成`[sin i,cos i]`，再将其拼接即可得到完整的时间向量。

以下代码在**TimeEmbedding.py**中实现：

```python
class TimeEmbedding(nn.Module):
    def __init__(self,d_model):
        super().__init__()

        # d_model是时间向量维度，在config中定义
        self.d_model=d_model
        # 半索引张量的大小
        self.half_size=d_model//2

        # 生成半索引张量    [0,1,2,...,d_model/2-1]
        i = torch.arange(self.half_size)
        # 计算k张量         k=2i/d_model
        k = i / (self.half_size)
        # 计算指数张量      j=1/10000^k
        j = torch.exp(k * (-1*math.log(10000)))
        # 结果存为buffer
        self.register_buffer('J',j)

    def forward(self,t):
        # 将 t 扩展成二维张量
        t=t.view(t.size(0),1)
        # 将系数 J 扩展为与t同批次大小的张量
        J=self.J.unsqueeze(0).expand(t.size(0),self.half_size)
        # 计算t时刻的时间嵌入系数
        t_index = J * t
        # 计算sin和cos值
        sin_vals = (t_index).sin()
        cos_vals = (t_index).cos()
        # 拼接sin和cos值，得到最终的时间嵌入
        embedding = torch.stack([sin_vals, cos_vals], dim=2)
        embedding = embedding.view(t.size(0), self.d_model)

        return embedding
```

## 单层卷积块

```python
from torch import nn

class ConvBlock(nn.Module):
    def __init__(self,in_channel,out_channel,time_emb_size):
        super().__init__()

        # 改通道数,不改大小
        self.seq1 = nn.Sequential(
            nn.Conv2d(
                in_channel,
                out_channel,
                kernel_size=3,
                stride=1,
                padding=1
                ), 
            nn.GroupNorm(32,out_channel),
            nn.SiLU(),
        )

        # Time时刻emb转成channel宽,加到每个像素点上
        self.time_emb_linear=nn.Linear(
            time_emb_size,
            out_channel
            )
        self.silu=nn.SiLU()

        # 不改通道数,不改大小
        self.seq2=nn.Sequential(
            nn.Conv2d(
                out_channel,
                out_channel,
                kernel_size=3,
                stride=1,
                padding=1
                ), 
            nn.GroupNorm(32,out_channel),
            nn.SiLU(),
        )


        # 残差结构,如果in_channel和out_channel不一样,用1x1卷积改通道数
        if in_channel != out_channel:
            self.residual = nn.Conv2d(
                in_channel,
                out_channel, 
                kernel_size=1
                ) 
        else:
            self.residual = nn.Identity()


    def forward(self,x,t_emb):

        # 计算残差
        residual = self.residual(x)

        # 改通道数,不改大小
        x=self.seq1(x)  

        # time_emb转成channel宽,加到每个像素点上
        t_emb=self.silu(
            self.time_emb_linear(t_emb)
            ).view(x.size(0),x.size(1),1,1)
        
        # 两者直接相加
        x=self.seq2(x+t_emb)

        # 残差相加
        output = x + residual
        return output
```


## Unet网络

```python
class UNet(nn.Module):
    def __init__(self,img_channel,channels=[64, 128, 256, 512, 1024],time_emb_size=256):
        super().__init__()

        channels=[img_channel]+channels
        
        # time转embedding
        self.time_emb=nn.Sequential(
            TimeEmbedding(time_emb_size),
            nn.Linear(time_emb_size,time_emb_size),
            nn.ReLU(),
        )

        # === encoder部分 === #

        # 每个encoder conv block增加一倍通道数
        self.enc_convs=nn.ModuleList()
        for i in range(len(channels)-1):
            self.enc_convs.append(ConvBlock(channels[i],channels[i+1],time_emb_size))
        
        # 每个encoder conv后马上缩小一倍图像尺寸,最后一个conv后不缩小
        self.maxpools=nn.ModuleList()
        for i in range(len(channels)-2):
            self.maxpools.append(nn.MaxPool2d(kernel_size=2,stride=2,padding=0))
        
        # === decoder部分 === #
        
        # 每个decoder conv前放大一倍图像尺寸，缩小一倍通道数
        self.deconvs=nn.ModuleList()
        for i in range(len(channels)-2):
            self.deconvs.append(nn.ConvTranspose2d(channels[-i-1],channels[-i-2],kernel_size=2,stride=2))

        # 每个decoder conv block减少一倍通道数
        self.dec_convs=nn.ModuleList()
        for i in range(len(channels)-2):
            self.dec_convs.append(ConvBlock(channels[-i-1],channels[-i-2],time_emb_size))   # 残差结构

        # 还原通道数,尺寸不变
        self.output=nn.Conv2d(channels[1],img_channel,kernel_size=1,stride=1,padding=0)

    def forward(self,x,t):
        # time做embedding
        t_emb=self.time_emb(t)
        
        # encoder阶段
        residual=[]
        for i,conv in enumerate(self.enc_convs):
            x=conv(x,t_emb)
            if i!=len(self.enc_convs)-1:
                residual.append(x)
                x=self.maxpools[i](x)
            
        # decoder阶段
        for i,deconv in enumerate(self.deconvs):
            x=deconv(x)
            residual_x=residual.pop(-1)
            x=self.dec_convs[i](torch.cat((residual_x,x),dim=1),t_emb)    # 残差用于纵深channel维
        return self.output(x) # 还原通道数
```


## 训练

```python
from config import *
from torch.utils.data import DataLoader
from dataset import user_dataset
from unet import UNet
from noise import forward_diffusion
import torch 
from torch import nn 
import os 
from tqdm import tqdm
from torch.optim.lr_scheduler import CosineAnnealingLR

# 数据加载器
dataloader=DataLoader(
    user_dataset,
    batch_size=BATCH_SIZE,
    num_workers=4,
    persistent_workers=True,
    shuffle=True
    )   

# 导入模型
try:
    model=torch.load('model.pt')
except:
    model=UNet(IMG_CHANNEL,time_emb_size=TIME_EMB_SIZE).to(DEVICE)

# 优化器
optimizer=torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)
# 学习率调度器
scheduler = CosineAnnealingLR(optimizer, T_max=EPOCH, eta_min=1e-6)
# 损失函数
loss_fn=nn.MSELoss() 
```
以下为所运行的main函数：
```python
if __name__=='__main__':
    # 加载模型
    model.train()

    # =======启动输出======== #
    print("开始训练")
    print(f"数据集样本数: {len(user_dataset)}")
    print(f"训练轮数: {EPOCH}, 批次大小: {BATCH_SIZE}, 每轮批次数: {len(dataloader)}")
    print(f"训练设备: {DEVICE}")
    print("模型结构:", model)

    # 循环每个EPOCH
    for epoch in range(EPOCH):
        # 初始化loss
        last_loss=0

        # 启用tqdm进度条
        loop = tqdm(
            enumerate(dataloader), 
            total=len(dataloader), 
            leave=True
            )
        loop.set_description(f"Epoch [{epoch+1}/{EPOCH}]")

        # 本轮的每个批次样本 (S, C, W. H)
        for i, data in loop:
            # 图像的像素范围转换到[-1,1],和高斯分布对应
            batch_x=data[0].to(DEVICE)*2-1
            # 为每张图片生成随机t时刻
            batch_t=torch.randint(0,T,(batch_x.size(0),)).to(DEVICE)
            # 生成t时刻的加噪图片和对应噪音
            batch_x_t,batch_noise_t=forward_diffusion(batch_x,batch_t)
            # 模型预测t时刻的噪音
            batch_predict_t=model(batch_x_t,batch_t)
            # 求损失
            loss=loss_fn(batch_predict_t,batch_noise_t)


            # 优化参数
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # 记录loss
            last_loss+=loss.item()
            loop.set_postfix(loss=loss.item())

        scheduler.step()

        epoch_loss = last_loss / len(dataloader)
        current_lr = scheduler.get_last_lr()[0]
        print(f'Epoch [{epoch+1}/{EPOCH}] Finished.  LR: {current_lr:.6f}\n, Average Loss: {epoch_loss:.4f}')



        torch.save(model,'model.pt.tmp')
        os.replace('model.pt.tmp','CIFAR_model.pt')
```

## 去噪

```python
import torch 
from config import *
from noise import *
import matplotlib.pyplot as plt 
from dataset import tensor_to_pil
from lora import LoraLayer
from torch import nn 
from lora import inject_lora

def backward_denoise(model, batch_x_t):
    steps = [batch_x_t,]

    # 模型+系数导入，同时迁移到同一设备
    model = model.to(DEVICE)
    global alphas, alphas_cumprod, betas
    batch_x_t = batch_x_t.to(DEVICE)
    alphas = alphas.to(DEVICE)
    alphas_cumprod = alphas_cumprod.to(DEVICE)
    betas = betas.to(DEVICE)
    
    model.eval()
    with torch.no_grad():
        for t in range(T-1, -1, -1):
            batch_t = torch.full((batch_x_t.size(0),), t).to(DEVICE)
            batch_predict_noise_t = model(batch_x_t, batch_t)
            
            shape = (batch_x_t.size(0), 1, 1, 1)
            batch_mean_t = 1 / torch.sqrt(alphas[batch_t].view(*shape)) * (
                batch_x_t - (1 - alphas[batch_t].view(*shape)) / 
                torch.sqrt(1 - alphas_cumprod[batch_t].view(*shape)) * batch_predict_noise_t
            )
            
            if t != 0:
                sigma_t = torch.sqrt(betas[batch_t].view(*shape))
                batch_x_t = batch_mean_t + torch.randn_like(batch_x_t) * sigma_t
            else:
                batch_x_t = batch_mean_t
                
            batch_x_t = torch.clamp(batch_x_t, -1.0, 1.0).detach()
            steps.append(batch_x_t)
    return steps
```


## 推理

```python

```